{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T05:18:04.332798Z",
     "start_time": "2025-08-20T05:17:11.076870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import speech_recognition as sr\n",
    "import pywhatkit\n",
    "from playsound import playsound\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename=r'C:\\Users\\sadik\\PyCharmMiscProject\\patient_room_monitoring.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "RECEIVER_PHONE = \"+919345531046\"  # Replace with actual phone number\n",
    "\n",
    "def send_whatsapp_alert(message):\n",
    "    try:\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        alert_message = (\n",
    "            f\"*** Patient Room Monitoring Alert ***\\n\\n\"\n",
    "            f\"Timestamp: {timestamp}\\n\"\n",
    "            f\"Alert Details: {message}\\n\\n\"\n",
    "            f\"Kindly address this situation promptly to ensure patient safety and well-being.\\n\"\n",
    "            f\"Thank you.\"\n",
    "        )\n",
    "        print(f\"Sending WhatsApp message:\\n{alert_message}\")\n",
    "        logging.info(f\"Sending WhatsApp message: {alert_message}\")\n",
    "        pywhatkit.sendwhatmsg_instantly(RECEIVER_PHONE, alert_message, wait_time=10, tab_close=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send WhatsApp message: {e}\")\n",
    "        logging.error(f\"Failed to send WhatsApp message: {e}\")\n",
    "\n",
    "def trigger_alarm(message, sound_file=None):\n",
    "    print(f\"ALARM: {message}\")\n",
    "    logging.info(f\"ALARM: {message}\")\n",
    "    send_whatsapp_alert(message)\n",
    "    if sound_file:\n",
    "        threading.Thread(target=playsound, args=(sound_file,), daemon=True).start()\n",
    "\n",
    "def adjust_gamma(image, gamma=0.5):\n",
    "    \"\"\"Apply gamma correction to the image to enhance contrast in low-light conditions.\"\"\"\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "# --- Night Mode Functions ---\n",
    "def detect_low_light(frame, threshold=60):\n",
    "    \"\"\"Check if the frame is in low-light condition.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    avg_brightness = np.mean(gray)\n",
    "    return avg_brightness < threshold\n",
    "\n",
    "def apply_night_mode(frame):\n",
    "    \"\"\"Enhance brightness and contrast for night conditions.\"\"\"\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v + 40, 0, 255)  # Increase brightness\n",
    "    s = np.clip(s - 30, 0, 255)  # Reduce saturation\n",
    "    hsv = cv2.merge((h, s, v))\n",
    "    frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    frame = adjust_gamma(frame, gamma=0.6)  # Increase contrast\n",
    "    return frame\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(r\"C:\\Users\\sadik\\PyCharmMiscProject\\yolov8n.pt\")\n",
    "\n",
    "# Load known faces\n",
    "known_faces_dir = r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\known_faces\"\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "for filename in os.listdir(known_faces_dir):\n",
    "    if filename.endswith((\".jpg\", \".png\")):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        image = face_recognition.load_image_file(image_path)\n",
    "        encoding = face_recognition.face_encodings(image)\n",
    "        if encoding:\n",
    "            known_face_encodings.append(encoding[0])\n",
    "            known_face_names.append(os.path.splitext(filename)[0])\n",
    "        else:\n",
    "            logging.warning(f\"No face encoding found in {filename}\")\n",
    "\n",
    "# MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "\n",
    "def count_fingers(hand_landmarks):\n",
    "    tips_ids = [4, 8, 12, 16, 20]\n",
    "    fingers = []\n",
    "    if hand_landmarks.landmark[tips_ids[0]].x < hand_landmarks.landmark[tips_ids[0] - 1].x:\n",
    "        fingers.append(1)\n",
    "    else:\n",
    "        fingers.append(0)\n",
    "    for id in range(1, 5):\n",
    "        if hand_landmarks.landmark[tips_ids[id]].y < hand_landmarks.landmark[tips_ids[id] - 2].y:\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "    return fingers.count(1)\n",
    "\n",
    "# Speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "\n",
    "ALARM_SOUNDS = {\n",
    "    \"call_nurse\": r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\alarm_sounds\\\\call_nurse.mp3\",\n",
    "    \"need_water\": r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\alarm_sounds\\\\need_water.mp3\",\n",
    "    \"call_family\": r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\alarm_sounds\\\\call_family.mp3\",\n",
    "    \"cancel_request\": r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\alarm_sounds\\\\cancel_request.mp3\",\n",
    "    \"unknown_alert\": r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\alarm_sounds\\\\unknown_alert.mp3\",\n",
    "    \"crowd_alert\": r\"C:\\\\Users\\\\sadik\\\\PyCharmMiscProject\\\\alarm_sounds\\\\crowd_alert.mp3\"\n",
    "}\n",
    "\n",
    "def listen_for_voice_commands():\n",
    "    with mic as source:\n",
    "        print(\"Adjusting for ambient noise... Please wait.\")\n",
    "        logging.info(\"Adjusting for ambient noise\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=5)\n",
    "        print(\"Listening for voice commands...\")\n",
    "        logging.info(\"Listening for voice commands\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                recognizer.dynamic_energy_threshold = True\n",
    "                recognizer.energy_threshold = 300\n",
    "                recognizer.pause_threshold = 1.0\n",
    "                recognizer.non_speaking_duration = 0.5\n",
    "\n",
    "                audio = recognizer.listen(source, timeout=8, phrase_time_limit=10)\n",
    "                try:\n",
    "                    command = recognizer.recognize_google(audio, language='en-US').lower()\n",
    "                    print(f\"Voice Command Detected: {command}\")\n",
    "                    logging.info(f\"Voice Command Detected: {command}\")\n",
    "\n",
    "                    if any(phrase in command for phrase in [\"call nurse\", \"nurse\", \"help\"]):\n",
    "                        trigger_alarm(\"Voice Command: Call Nurse\", ALARM_SOUNDS[\"call_nurse\"])\n",
    "                    elif any(phrase in command for phrase in [\"need water\", \"water\", \"thirsty\"]):\n",
    "                        trigger_alarm(\"Voice Command: Need Water\", ALARM_SOUNDS[\"need_water\"])\n",
    "                    elif any(phrase in command for phrase in [\"call family\", \"family\", \"contact family\"]):\n",
    "                        trigger_alarm(\"Voice Command: Call Family\", ALARM_SOUNDS[\"call_family\"])\n",
    "                    elif any(phrase in command for phrase in [\"cancel\", \"stop\", \"abort\"]):\n",
    "                        trigger_alarm(\"Voice Command: Cancel Request\", ALARM_SOUNDS[\"cancel_request\"])\n",
    "                    else:\n",
    "                        print(f\"Unrecognized command: {command}\")\n",
    "                        logging.warning(f\"Unrecognized command: {command}\")\n",
    "\n",
    "                except sr.UnknownValueError:\n",
    "                    print(\"Could not understand the audio, please try again.\")\n",
    "                    logging.warning(\"Could not understand the audio\")\n",
    "                except sr.RequestError as e:\n",
    "                    print(f\"Speech recognition service error: {e}\")\n",
    "                    logging.error(f\"Speech recognition service error: {e}\")\n",
    "                    time.sleep(2)\n",
    "\n",
    "            except sr.WaitTimeoutError:\n",
    "                print(\"No voice detected within timeout, listening again...\")\n",
    "                logging.info(\"No voice detected within timeout\")\n",
    "                recognizer.adjust_for_ambient_noise(source, duration=2)\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error in voice recognition: {e}\")\n",
    "                logging.error(f\"Unexpected error in voice recognition: {e}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "def start_voice_thread():\n",
    "    voice_thread = threading.Thread(target=listen_for_voice_commands, daemon=True)\n",
    "    voice_thread.start()\n",
    "    return voice_thread\n",
    "\n",
    "# Modes\n",
    "mode = \"default\"\n",
    "mode_set_time = None\n",
    "cooldown_seconds = 10\n",
    "\n",
    "gesture_detected_time = {}\n",
    "gesture_labels = {0: \"Call Nurse\", 2: \"Need Water\", 3: \"Call Family\"}\n",
    "gesture_sounds = {0: \"call_nurse\", 2: \"need_water\", 3: \"call_family\"}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame from camera\")\n",
    "        logging.error(\"Failed to capture frame from camera\")\n",
    "        break\n",
    "\n",
    "    # --- Night Mode Auto Detection ---\n",
    "    if detect_low_light(frame):\n",
    "        frame = apply_night_mode(frame)\n",
    "        cv2.putText(frame, \"Night Mode\", (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "    else:\n",
    "        frame = adjust_gamma(frame, gamma=0.5)\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = model(rgb_frame)\n",
    "    person_count = 0\n",
    "    unknown_face_detected = False\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            if int(box.cls) == 0:\n",
    "                person_count += 1\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                person_roi = rgb_frame[y1:y2, x1:x2]\n",
    "                face_locations = face_recognition.face_locations(person_roi)\n",
    "\n",
    "                if face_locations:\n",
    "                    top, right, bottom, left = face_locations[0]\n",
    "                    top += y1\n",
    "                    bottom += y1\n",
    "                    left += x1\n",
    "                    right += x1\n",
    "                    face_encoding = face_recognition.face_encodings(rgb_frame, [(top, right, bottom, left)])\n",
    "                    if face_encoding:\n",
    "                        matches = face_recognition.compare_faces(known_face_encodings, face_encoding[0])\n",
    "                        name = \"Unknown\"\n",
    "                        if True in matches:\n",
    "                            name = known_face_names[matches.index(True)]\n",
    "                        else:\n",
    "                            unknown_face_detected = True\n",
    "\n",
    "                        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                        cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    results_hands = hands.process(rgb_frame)\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            fingers_up = count_fingers(hand_landmarks)\n",
    "            cv2.putText(frame, f\"Fingers: {fingers_up}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            if mode_set_time is None or (time.time() - mode_set_time > cooldown_seconds):\n",
    "                if fingers_up == 5 and mode != \"default\":\n",
    "                    mode = \"default\"\n",
    "                    mode_set_time = time.time()\n",
    "                    print(\"Switched to Default Monitoring Mode\")\n",
    "                    logging.info(\"Switched to Default Monitoring Mode\")\n",
    "\n",
    "                elif fingers_up == 4 and mode != \"gesture\":\n",
    "                    mode = \"gesture\"\n",
    "                    mode_set_time = time.time()\n",
    "                    print(\"Gesture Mode Activated\")\n",
    "                    logging.info(\"Gesture Mode Activated\")\n",
    "\n",
    "                elif fingers_up == 1 and mode != \"voice\":\n",
    "                    mode = \"voice\"\n",
    "                    mode_set_time = time.time()\n",
    "                    start_voice_thread()\n",
    "                    print(\"Voice Recognition Mode Activated\")\n",
    "                    logging.info(\"Voice Recognition Mode Activated\")\n",
    "\n",
    "            if mode == \"gesture\" and fingers_up in gesture_labels:\n",
    "                gesture = gesture_labels[fingers_up]\n",
    "                cv2.putText(frame, f\"Gesture: {gesture}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 0), 3)\n",
    "\n",
    "                if fingers_up not in gesture_detected_time:\n",
    "                    gesture_detected_time[fingers_up] = time.time()\n",
    "                elif time.time() - gesture_detected_time[fingers_up] > 2:\n",
    "                    trigger_alarm(f\"Gesture: {gesture}\", ALARM_SOUNDS[gesture_sounds[fingers_up]])\n",
    "                    gesture_detected_time.pop(fingers_up)\n",
    "            else:\n",
    "                gesture_detected_time.clear()\n",
    "    else:\n",
    "        gesture_detected_time.clear()\n",
    "\n",
    "    cv2.putText(frame, f\"People: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"Mode: {mode.upper()}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 100), 2)\n",
    "\n",
    "    if unknown_face_detected:\n",
    "        print(\"Unknown face detected!\")\n",
    "        logging.warning(\"Unknown face detected\")\n",
    "        threading.Thread(target=playsound, args=(ALARM_SOUNDS[\"unknown_alert\"],), daemon=True).start()\n",
    "\n",
    "    if person_count > 50:\n",
    "        trigger_alarm(f\"Too many people: {person_count}\", ALARM_SOUNDS[\"crowd_alert\"])\n",
    "\n",
    "    cv2.imshow(\"Patient Room Monitoring\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "6666c26879680353",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadik\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition_models\\__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 tie, 62.4ms\n",
      "Speed: 2.4ms preprocess, 62.4ms inference, 85.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 13.4ms\n",
      "Speed: 2.3ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.1ms\n",
      "Speed: 1.1ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.9ms\n",
      "Speed: 1.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.2ms\n",
      "Speed: 2.9ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 14.4ms\n",
      "Speed: 1.2ms preprocess, 14.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.5ms\n",
      "Speed: 1.3ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.7ms\n",
      "Speed: 1.1ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 11.7ms\n",
      "Speed: 1.9ms preprocess, 11.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.6ms\n",
      "Speed: 1.1ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 7.0ms\n",
      "Speed: 1.1ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 15.7ms\n",
      "Speed: 2.3ms preprocess, 15.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.1ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 7.0ms\n",
      "Speed: 1.2ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.9ms\n",
      "Speed: 1.4ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 15.8ms\n",
      "Speed: 1.7ms preprocess, 15.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.3ms\n",
      "Speed: 1.2ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.2ms\n",
      "Speed: 1.1ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.4ms\n",
      "Speed: 1.2ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.4ms\n",
      "Speed: 1.6ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.2ms\n",
      "Speed: 1.2ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 15.8ms\n",
      "Speed: 1.6ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 7.0ms\n",
      "Speed: 1.1ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.9ms\n",
      "Speed: 1.2ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.1ms\n",
      "Speed: 1.1ms preprocess, 7.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 13.2ms\n",
      "Speed: 1.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 13.3ms\n",
      "Speed: 2.0ms preprocess, 13.3ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 14.2ms\n",
      "Speed: 1.7ms preprocess, 14.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 27.4ms\n",
      "Speed: 3.4ms preprocess, 27.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 26.0ms\n",
      "Speed: 3.6ms preprocess, 26.0ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 22.6ms\n",
      "Speed: 3.0ms preprocess, 22.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 29.7ms\n",
      "Speed: 2.9ms preprocess, 29.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.4ms\n",
      "Speed: 3.2ms preprocess, 31.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 24.9ms\n",
      "Speed: 3.6ms preprocess, 24.9ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 32.1ms\n",
      "Speed: 2.9ms preprocess, 32.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 33.6ms\n",
      "Speed: 3.1ms preprocess, 33.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 28.7ms\n",
      "Speed: 3.4ms preprocess, 28.7ms inference, 6.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 4.4ms preprocess, 37.7ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.0ms\n",
      "Speed: 5.4ms preprocess, 35.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.8ms\n",
      "Speed: 3.6ms preprocess, 38.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 37.7ms\n",
      "Speed: 2.8ms preprocess, 37.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 4.6ms preprocess, 38.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.5ms\n",
      "Speed: 2.9ms preprocess, 36.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 3.4ms preprocess, 37.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.0ms\n",
      "Speed: 4.2ms preprocess, 41.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.4ms\n",
      "Speed: 3.0ms preprocess, 42.4ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 3.0ms preprocess, 38.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.8ms\n",
      "Speed: 3.2ms preprocess, 37.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 3.0ms preprocess, 38.0ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.9ms\n",
      "Speed: 4.8ms preprocess, 35.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.8ms\n",
      "Speed: 3.6ms preprocess, 36.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 3.3ms preprocess, 39.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.3ms\n",
      "Speed: 5.4ms preprocess, 40.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 3.0ms preprocess, 38.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.5ms\n",
      "Speed: 3.5ms preprocess, 40.5ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 4.1ms preprocess, 37.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 3.6ms preprocess, 39.0ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.4ms\n",
      "Speed: 3.5ms preprocess, 38.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.6ms\n",
      "Speed: 3.3ms preprocess, 37.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.7ms\n",
      "Speed: 4.5ms preprocess, 39.7ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.3ms\n",
      "Speed: 2.8ms preprocess, 38.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.7ms\n",
      "Speed: 3.1ms preprocess, 40.7ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 3.1ms preprocess, 37.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 3.4ms preprocess, 38.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.8ms\n",
      "Speed: 5.8ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.8ms\n",
      "Speed: 2.7ms preprocess, 37.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.4ms\n",
      "Speed: 3.0ms preprocess, 31.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 3.2ms preprocess, 38.0ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.7ms\n",
      "Speed: 3.8ms preprocess, 31.7ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.6ms\n",
      "Speed: 8.1ms preprocess, 35.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.6ms\n",
      "Speed: 5.3ms preprocess, 45.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 2.9ms preprocess, 37.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.4ms\n",
      "Speed: 3.3ms preprocess, 39.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.8ms\n",
      "Speed: 3.1ms preprocess, 39.8ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 2.9ms preprocess, 38.5ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.5ms\n",
      "Speed: 3.4ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 3.1ms preprocess, 38.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.4ms\n",
      "Speed: 4.3ms preprocess, 41.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 44.1ms\n",
      "Speed: 2.9ms preprocess, 44.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 6.6ms preprocess, 37.1ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.7ms\n",
      "Speed: 3.6ms preprocess, 45.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.4ms\n",
      "Speed: 3.7ms preprocess, 31.4ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 46.1ms\n",
      "Speed: 3.1ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.6ms\n",
      "Speed: 4.0ms preprocess, 36.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 3.7ms preprocess, 39.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.1ms\n",
      "Speed: 3.2ms preprocess, 38.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.8ms\n",
      "Speed: 3.5ms preprocess, 39.8ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.7ms\n",
      "Speed: 6.8ms preprocess, 31.7ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.4ms\n",
      "Speed: 6.2ms preprocess, 45.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.8ms\n",
      "Speed: 3.4ms preprocess, 37.8ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 31.8ms\n",
      "Speed: 3.6ms preprocess, 31.8ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 34.6ms\n",
      "Speed: 8.5ms preprocess, 34.6ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.2ms\n",
      "Speed: 8.2ms preprocess, 38.2ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.5ms\n",
      "Speed: 3.5ms preprocess, 36.5ms inference, 9.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 5.1ms preprocess, 37.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 38.2ms\n",
      "Speed: 5.0ms preprocess, 38.2ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.1ms\n",
      "Speed: 6.4ms preprocess, 41.1ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.0ms\n",
      "Speed: 6.5ms preprocess, 36.0ms inference, 6.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.2ms\n",
      "Speed: 4.5ms preprocess, 36.2ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.7ms\n",
      "Speed: 4.0ms preprocess, 36.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 35.6ms\n",
      "Speed: 4.1ms preprocess, 35.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 38.6ms\n",
      "Speed: 6.2ms preprocess, 38.6ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 217\u001B[0m\n\u001B[0;32m    215\u001B[0m cv2\u001B[38;5;241m.\u001B[39mrectangle(frame, (x1, y1), (x2, y2), (\u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    216\u001B[0m person_roi \u001B[38;5;241m=\u001B[39m rgb_frame[y1:y2, x1:x2]\n\u001B[1;32m--> 217\u001B[0m face_locations \u001B[38;5;241m=\u001B[39m \u001B[43mface_recognition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mface_locations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mperson_roi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m face_locations:\n\u001B[0;32m    220\u001B[0m     top, right, bottom, left \u001B[38;5;241m=\u001B[39m face_locations[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition\\api.py:121\u001B[0m, in \u001B[0;36mface_locations\u001B[1;34m(img, number_of_times_to_upsample, model)\u001B[0m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [_trim_css_to_bounds(_rect_to_css(face\u001B[38;5;241m.\u001B[39mrect), img\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m face \u001B[38;5;129;01min\u001B[39;00m _raw_face_locations(img, number_of_times_to_upsample, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcnn\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [_trim_css_to_bounds(_rect_to_css(face), img\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m face \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_raw_face_locations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_times_to_upsample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition\\api.py:105\u001B[0m, in \u001B[0;36m_raw_face_locations\u001B[1;34m(img, number_of_times_to_upsample, model)\u001B[0m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mface_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_times_to_upsample\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T20:33:57.081764Z",
     "start_time": "2025-08-15T20:33:56.995739Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "976180f5128f131",
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\windows\\\\system32\\\\patient_room_monitoring.log'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 291\u001B[0m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    290\u001B[0m     config \u001B[38;5;241m=\u001B[39m Config()\n\u001B[1;32m--> 291\u001B[0m     monitor \u001B[38;5;241m=\u001B[39m \u001B[43mPatientMonitor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    292\u001B[0m     monitor\u001B[38;5;241m.\u001B[39mrun()\n",
      "Cell \u001B[1;32mIn[8], line 55\u001B[0m, in \u001B[0;36mPatientMonitor.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config):\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m---> 55\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup_logging\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mfilterwarnings(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[1;32mIn[8], line 82\u001B[0m, in \u001B[0;36mPatientMonitor.setup_logging\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msetup_logging\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 82\u001B[0m     \u001B[43mlogging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbasicConfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLOG_FILE_PATH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mINFO\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m%(asctime)s\u001B[39;49;00m\u001B[38;5;124;43m - \u001B[39;49m\u001B[38;5;132;43;01m%(levelname)s\u001B[39;49;00m\u001B[38;5;124;43m - \u001B[39;49m\u001B[38;5;132;43;01m%(message)s\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[0;32m     86\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogging to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mLOG_FILE_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py:1991\u001B[0m, in \u001B[0;36mbasicConfig\u001B[1;34m(**kwargs)\u001B[0m\n\u001B[0;32m   1989\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1990\u001B[0m         errors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1991\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[43mFileHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1992\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1993\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1994\u001B[0m     stream \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py:1142\u001B[0m, in \u001B[0;36mFileHandler.__init__\u001B[1;34m(self, filename, mode, encoding, delay, errors)\u001B[0m\n\u001B[0;32m   1140\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1141\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1142\u001B[0m     StreamHandler\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py:1171\u001B[0m, in \u001B[0;36mFileHandler._open\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1166\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_open\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1167\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1168\u001B[0m \u001B[38;5;124;03m    Open the current base file with the (original) mode and encoding.\u001B[39;00m\n\u001B[0;32m   1169\u001B[0m \u001B[38;5;124;03m    Return the resulting stream.\u001B[39;00m\n\u001B[0;32m   1170\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbaseFilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[43m                \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'C:\\\\windows\\\\system32\\\\patient_room_monitoring.log'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T20:24:21.666211Z",
     "start_time": "2025-08-15T20:24:21.582214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "import bson  # For handling binary data like images\n",
    "import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# Make sure this matches the settings in your main application\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "MONGO_DB_NAME = \"patient_monitoring\"\n",
    "MONGO_COLLECTION_NAME = \"patients\"\n",
    "\n",
    "# --- Patient Information ---\n",
    "# This is the data for the new patient you want to add.\n",
    "# MongoDB uses '_id' as the unique identifier for a document.\n",
    "patient_id = \"PAT001\"\n",
    "patient_name = \"John Doe\"\n",
    "patient_age = 76\n",
    "patient_gender = \"Male\"\n",
    "# IMPORTANT: Make sure you have an image file with this name in the same folder as the script.\n",
    "patient_photo_path =r\"C:\\Users\\sadik\\PyCharmMiscProject\\tamizh.jpg\"\n",
    "\n",
    "def add_patient_to_db():\n",
    "    \"\"\"Connects to MongoDB and inserts a new patient document.\"\"\"\n",
    "\n",
    "    # 1. Connect to the database\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.admin.command('ismaster') # Check if the connection is successful\n",
    "        db = client[MONGO_DB_NAME]\n",
    "        collection = db[MONGO_COLLECTION_NAME]\n",
    "        print(f\"Successfully connected to MongoDB database: '{MONGO_DB_NAME}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not connect to MongoDB. {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Check if the patient photo exists\n",
    "    if not os.path.exists(patient_photo_path):\n",
    "        print(f\"Error: Photo file not found at '{patient_photo_path}'. Please provide a valid image file.\")\n",
    "        # Create a placeholder file if it doesn't exist, so the user can replace it.\n",
    "        with open(patient_photo_path, 'wb') as f:\n",
    "            f.write(b'') # Write empty bytes\n",
    "        print(f\"A placeholder file has been created. Please replace '{patient_photo_path}' with the patient's actual photo.\")\n",
    "        return\n",
    "\n",
    "    # 3. Read the image file as binary data\n",
    "    try:\n",
    "        with open(patient_photo_path, 'rb') as image_file:\n",
    "            photo_data = image_file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading image file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Create the patient document\n",
    "    patient_document = {\n",
    "        \"_id\": patient_id,  # Setting a custom ID\n",
    "        \"name\": patient_name,\n",
    "        \"age\": patient_age,\n",
    "        \"gender\": patient_gender,\n",
    "        \"photo\": bson.binary.Binary(photo_data), # Store the image data in binary format\n",
    "        \"registration_date\": datetime.datetime.utcnow()\n",
    "    }\n",
    "\n",
    "    # 5. Insert the document into the collection\n",
    "    try:\n",
    "        # Check if a patient with this ID already exists to avoid duplicates\n",
    "        if collection.find_one({\"_id\": patient_id}):\n",
    "            print(f\"Patient with ID '{patient_id}' already exists. No new record was added.\")\n",
    "        else:\n",
    "            collection.insert_one(patient_document)\n",
    "            print(f\"Successfully added patient '{patient_name}' with ID '{patient_id}' to the '{MONGO_COLLECTION_NAME}' collection.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting document into MongoDB: {e}\")\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_patient_to_db()\n"
   ],
   "id": "b9d282a48c7d076d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB database: 'patient_monitoring'\n",
      "Successfully added patient 'John Doe' with ID 'PAT001' to the 'patients' collection.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T20:28:01.918032Z",
     "start_time": "2025-08-15T20:28:01.736931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from PIL import Image # Pillow library is great for handling images\n",
    "import io\n",
    "\n",
    "# --- Configuration ---\n",
    "# Make sure this matches the settings in your main application\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "MONGO_DB_NAME = \"patient_monitoring\"\n",
    "MONGO_COLLECTION_NAME = \"patients\"\n",
    "\n",
    "def fetch_patient_data(patient_id_to_find):\n",
    "    \"\"\"Connects to MongoDB, fetches a patient document by its ID, and saves the photo.\"\"\"\n",
    "\n",
    "    # 1. Connect to the database\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.admin.command('ismaster') # Check if the connection is successful\n",
    "        db = client[MONGO_DB_NAME]\n",
    "        collection = db[MONGO_COLLECTION_NAME]\n",
    "        print(f\"Successfully connected to MongoDB database: '{MONGO_DB_NAME}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not connect to MongoDB. {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Fetch the document from the collection\n",
    "    try:\n",
    "        print(f\"\\nSearching for patient with ID: {patient_id_to_find}...\")\n",
    "        patient_document = collection.find_one({\"_id\": patient_id_to_find})\n",
    "\n",
    "        if patient_document:\n",
    "            print(\"--- Patient Found ---\")\n",
    "            print(f\"ID: {patient_document.get('_id')}\")\n",
    "            print(f\"Name: {patient_document.get('name')}\")\n",
    "            print(f\"Age: {patient_document.get('age')}\")\n",
    "            print(f\"Gender: {patient_document.get('gender')}\")\n",
    "            print(f\"Registration Date: {patient_document.get('registration_date')}\")\n",
    "\n",
    "            # 3. Handle and save the photo\n",
    "            photo_data = patient_document.get('photo')\n",
    "            if photo_data and isinstance(photo_data, bytes):\n",
    "                try:\n",
    "                    # Use Pillow to open the image from the binary data\n",
    "                    image = Image.open(io.BytesIO(photo_data))\n",
    "\n",
    "                    # Define the output path for the saved photo\n",
    "                    output_filename = f\"retrieved_{patient_id_to_find}.jpg\"\n",
    "\n",
    "                    # Save the image to a file\n",
    "                    image.save(output_filename)\n",
    "                    print(f\"\\nSuccessfully saved patient photo as '{output_filename}'\")\n",
    "\n",
    "                    # Optional: Display the image\n",
    "                    # image.show()\n",
    "\n",
    "                except Exception as img_e:\n",
    "                    print(f\"Error processing image data: {img_e}\")\n",
    "            else:\n",
    "                print(\"No photo data found for this patient.\")\n",
    "\n",
    "        else:\n",
    "            print(\"-----------------------\")\n",
    "            print(\"Patient not found.\")\n",
    "            print(\"-----------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "    finally:\n",
    "        # 4. Close the connection\n",
    "        client.close()\n",
    "        print(\"\\nDatabase connection closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The ID of the patient you want to retrieve\n",
    "    patient_id_to_fetch = \"PAT001\"\n",
    "    fetch_patient_data(patient_id_to_fetch)\n"
   ],
   "id": "10b42b69acf84550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB database: 'patient_monitoring'\n",
      "\n",
      "Searching for patient with ID: PAT001...\n",
      "--- Patient Found ---\n",
      "ID: PAT001\n",
      "Name: John Doe\n",
      "Age: 76\n",
      "Gender: Male\n",
      "Registration Date: 2025-08-15 20:24:21.619000\n",
      "Error processing image data: [Errno 13] Permission denied: 'retrieved_PAT001.jpg'\n",
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
